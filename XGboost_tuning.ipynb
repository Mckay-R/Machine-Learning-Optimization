{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tuning XGBoost: A practical outlook of three popular hyperparameter tuning methods**\n",
    "### **Important concepts:**\n",
    "\n",
    "**What is an XGBoost?** \n",
    "\n",
    "XGBoost is a highly scalable and distributed machine learning algorithm that leverages gradient boosting and decision trees to address a wide range of problems, including regression, classification, and ranking. XGBoost, which is short for Extreme Gradient Boosting, gets its \"extreme\" label because it employs advanced regularization techniques to enhance the performance of gradient boosting. XGBoost was developed with the specific goal of making gradient boosting more versatile and regularized across various applications.\n",
    "\n",
    "**What are hyperparameters?**\n",
    "\n",
    "Hyperparameters are external model parameter settings whose values cannot be learned from the data during training. These parameters are configured manually before initiating the training process, and they remain unaffected by the training itself.\n",
    "\n",
    "**What is Hyperparameter Tuning?** \n",
    "\n",
    "Hyperparameter tuning involves the quest for the ideal hyperparameter values that can maximize a machine learning algorithm's performance. The primary objectives are to boost a model's predictive precision and potentially optimize its computational efficiency. It's essential to note that the optimal hyperparameters can be subjective, varying from one dataset to another, and it's the responsibility of the model developer to specify these hyperparameters, as they are fundamental settings for all machine learning models.\n",
    "\n",
    "**Hyperparameters in XGBoost**\n",
    "\n",
    "In XGboost, there are two main categories of hyperparameters: \n",
    "\n",
    "* tree booster hyperparameters\n",
    "* learning task hyperparameters\n",
    "\n",
    "Hyperparameters related to tree booster govern the construction and intricacy of the decision trees within the model. Examples of tree booster parameters include: \n",
    "\n",
    "* *max_depth:* Maximum depth of a tree increases the model complexity. A model with a very high max_depth will most likely overfits. Default value is 6. \n",
    "\n",
    "* *subsample:* Subsample ratio of the training instances. Setting it to 0.5 means XGboost would randomly sample half of the training data prior to growing trees, and this helps prevent overfitting. Subsampling will occur once in every boosting iteration. Default value is 1.\n",
    "\n",
    "Hyperparameters related to learning task play a pivotal role in shaping both the model's behavior and the entire learning process. Examples of such are: \n",
    "\n",
    "* *learning_rate:* Step size shrinkage used in update to prevent overfitting. After each boosting step, we can directly get the weights of new\n",
    "features, and learning rate shrinks the feature weights to make the boosting process more conservative. Lower values make the model more robust by taking smaller steps. Default value is 0.3\n",
    "\n",
    "* *alpha:* L1 regularization term on weights. Increasing this value will make model more conservation. *alpha* ranges from 0 to postive infinity. Default value is 0\n",
    "\n",
    "* *lambda:* L2 regularization term on weights. Increasing this value will make model more conservation. *lambda* ranges from 0 to postive infinity. Default value is 1.\n",
    "\n",
    "**Methods of Hyperparameter tuning**\n",
    "\n",
    "In this project, I have explored and compared only three hyperparameter methods. And of course, there are other methods beyond these three. The three includes:\n",
    "\n",
    "* Grid Search: This is a systematic and automated method for hyperparameter tuning. Think of it as a way of finding the best combination of hyperparameters for your model without having to manually try different values one by one. The time and resources to run every combinations becomes increase as the combination increases. Hence, it is very time consuming and inefficient for a production task. \n",
    "\n",
    "* Randomized Search: This is an alternative approach to Grid Search, which exhaustively tries all possible combinations of hyperparameter values. This method performs a random selection/combination of hyperparameters for find the optimal values. The downside to using this method might be the inability to find the optimal hyperparameters. \n",
    "\n",
    "* Optuna:  Optuna is a sophisticated technique for hyperparameter tuning, employing a Bayesian optimization approach. It harnesses the power of Bayesian reasoning by calculating probabilities to pinpoint the best hyperparameter values, which efficiently reduces computational overhead by eliminating combinations of parameters that are not contributing to model performance. Optuna stands out for its effectiveness in both sampling potential hyperparameters and pruning out less promising ones, making it a valuable tool for enhancing the efficiency of hyperparameter optimization.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "random.seed(123)\n",
    "import time\n",
    "#--------------------------------------------------------------------------------------------\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "#---------------------------------------------------------------------------------------------\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score  \n",
    "from pprint import pprint\n",
    "#---------------------------------------------------------------------------------------------\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "#----------------------------------------------------------------------------------------------\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Import data**\n",
    "\n",
    "For this project, I have used the wholesale customers data, downloaded from kaggle. The dataset refers to clients of a wholesale distributor. It includes the annual spending in monetary units (m.u) on diverse product categories. Each model in this notebook aim to predict the category to which customers belong - Hotel/Restaurant/Cafe OR Retail channel. Hence, this is a classification problem.\n",
    "\n",
    "Two things to note about the target variable:\n",
    "\n",
    "* The target variable is unbalanced, that is, one class has more instance than the other\n",
    "* No sampling technique is used to adjust for class imbalance. The data is used as is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Region</th>\n",
       "      <th>Fresh</th>\n",
       "      <th>Milk</th>\n",
       "      <th>Grocery</th>\n",
       "      <th>Frozen</th>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <th>Delicassen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12669</td>\n",
       "      <td>9656</td>\n",
       "      <td>7561</td>\n",
       "      <td>214</td>\n",
       "      <td>2674</td>\n",
       "      <td>1338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7057</td>\n",
       "      <td>9810</td>\n",
       "      <td>9568</td>\n",
       "      <td>1762</td>\n",
       "      <td>3293</td>\n",
       "      <td>1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6353</td>\n",
       "      <td>8808</td>\n",
       "      <td>7684</td>\n",
       "      <td>2405</td>\n",
       "      <td>3516</td>\n",
       "      <td>7844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13265</td>\n",
       "      <td>1196</td>\n",
       "      <td>4221</td>\n",
       "      <td>6404</td>\n",
       "      <td>507</td>\n",
       "      <td>1788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>22615</td>\n",
       "      <td>5410</td>\n",
       "      <td>7198</td>\n",
       "      <td>3915</td>\n",
       "      <td>1777</td>\n",
       "      <td>5185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Channel  Region  Fresh  Milk  Grocery  Frozen  Detergents_Paper  Delicassen\n",
       "0        2       3  12669  9656     7561     214              2674        1338\n",
       "1        2       3   7057  9810     9568    1762              3293        1776\n",
       "2        2       3   6353  8808     7684    2405              3516        7844\n",
       "3        1       3  13265  1196     4221    6404               507        1788\n",
       "4        2       3  22615  5410     7198    3915              1777        5185"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('C:/Users/oyeni/Projects/XGboost/Data/Wholesale customers data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Define independent and dependent variables/Split data into train and test sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape (308, 7) (308,)\n",
      "Testing Data Shape (132, 7) (132,)\n"
     ]
    }
   ],
   "source": [
    "#create dependent and independent variables \n",
    "X = data.drop('Channel', axis = 1)\n",
    "y = data['Channel']\n",
    "\n",
    "#convert labels into binary values\n",
    "y[y==2] = 0\n",
    "y[y==1] = 1\n",
    "\n",
    "#split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0)\n",
    "\n",
    "print('Training Data Shape', X_train.shape, y_train.shape)\n",
    "print('Testing Data Shape', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Define functions for data summary and model evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_statistics(df, response):\n",
    "    \"\"\"Returns the data's different statistics.\n",
    "        Parameters:\n",
    "              X: independent variables \n",
    "              y: dependent variable\n",
    "    \n",
    "        Returns: \n",
    "              Top 5 rows of data\n",
    "              Five number summary\n",
    "              Number of unique elements per column\n",
    "              The shape of the data\n",
    "              Data type information\n",
    "              distribution of dependent variable\n",
    "              Dependent variable label count\n",
    "              missing data\n",
    "    \"\"\"\n",
    "    print(f\"Top 5 rows of data: \\n{df.head(5)}\")\n",
    "    print(\"---------------------------------------------------------------------------------------------------------------------\")\n",
    "    print(f\"Five number summary: \\n{df.describe()}\")\n",
    "    print(\"---------------------------------------------------------------------------------------------------------------------\")\n",
    "    print(f\"Number of unique element per column: \\n{df.nunique()}\")\n",
    "    print(\"---------------------------------------------------------------------------------------------------------------------\")\n",
    "    print(f\"Shape of the data is : \\n{df.shape}\")\n",
    "    print(\"---------------------------------------------------------------------------------------------------------------------\")\n",
    "    print(df.info())\n",
    "    print(\"---------------------------------------------------------------------------------------------------------------------\")\n",
    "    print(f\"Distribution of dependent variable: \\n{df[response].value_counts()}\")\n",
    "    print(\"---------------------------------------------------------------------------------------------------------------------\")\n",
    "    print(f\"Unique label in dependent variable: {sorted(df[response].unique())}\")\n",
    "    print(\"---------------------------------------------------------------------------------------------------------------------\")\n",
    "    print(f\"How many data are missing: \\n{df.isnull().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_train(model):\n",
    "    \"\"\"\n",
    "    Return predictions, precision, recall and F1_score obtained on train data.\n",
    "        Parameters: \n",
    "              model: the model used\n",
    "        \n",
    "        Returns:\n",
    "              Predictions, precision, recall and F1_score on train data\n",
    "\n",
    "    \"\"\"\n",
    "    pred = model.predict(X_train)\n",
    "    precision = precision_score(y_train, pred)\n",
    "    recall = recall_score(y_train, pred)\n",
    "    F1_score = f1_score(y_train, pred)\n",
    "    \n",
    "    return pred, precision, recall, F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(model):\n",
    "    \"\"\"\n",
    "    Return predictions, precision, recall and F1_score obtained on test data.\n",
    "        Parameters: \n",
    "              model: the model used\n",
    "        \n",
    "        Returns:\n",
    "              Predictions, precision, recall and F1_score on train data\n",
    "\n",
    "    \"\"\"\n",
    "    pred = model.predict(X_test)\n",
    "    precision = precision_score(y_test, pred)\n",
    "    recall = recall_score(y_test, pred)\n",
    "    F1_score = f1_score(y_test, pred)\n",
    "    \n",
    "    return pred, precision, recall, F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 rows of data: \n",
      "   Channel  Region  Fresh  Milk  Grocery  Frozen  Detergents_Paper  Delicassen\n",
      "0        0       3  12669  9656     7561     214              2674        1338\n",
      "1        0       3   7057  9810     9568    1762              3293        1776\n",
      "2        0       3   6353  8808     7684    2405              3516        7844\n",
      "3        1       3  13265  1196     4221    6404               507        1788\n",
      "4        0       3  22615  5410     7198    3915              1777        5185\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Five number summary: \n",
      "          Channel      Region          Fresh          Milk       Grocery  \\\n",
      "count  440.000000  440.000000     440.000000    440.000000    440.000000   \n",
      "mean     0.677273    2.543182   12000.297727   5796.265909   7951.277273   \n",
      "std      0.468052    0.774272   12647.328865   7380.377175   9503.162829   \n",
      "min      0.000000    1.000000       3.000000     55.000000      3.000000   \n",
      "25%      0.000000    2.000000    3127.750000   1533.000000   2153.000000   \n",
      "50%      1.000000    3.000000    8504.000000   3627.000000   4755.500000   \n",
      "75%      1.000000    3.000000   16933.750000   7190.250000  10655.750000   \n",
      "max      1.000000    3.000000  112151.000000  73498.000000  92780.000000   \n",
      "\n",
      "             Frozen  Detergents_Paper    Delicassen  \n",
      "count    440.000000        440.000000    440.000000  \n",
      "mean    3071.931818       2881.493182   1524.870455  \n",
      "std     4854.673333       4767.854448   2820.105937  \n",
      "min       25.000000          3.000000      3.000000  \n",
      "25%      742.250000        256.750000    408.250000  \n",
      "50%     1526.000000        816.500000    965.500000  \n",
      "75%     3554.250000       3922.000000   1820.250000  \n",
      "max    60869.000000      40827.000000  47943.000000  \n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Number of unique element per column: \n",
      "Channel               2\n",
      "Region                3\n",
      "Fresh               433\n",
      "Milk                421\n",
      "Grocery             430\n",
      "Frozen              426\n",
      "Detergents_Paper    417\n",
      "Delicassen          403\n",
      "dtype: int64\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Shape of the data is : \n",
      "(440, 8)\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 440 entries, 0 to 439\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count  Dtype\n",
      "---  ------            --------------  -----\n",
      " 0   Channel           440 non-null    int64\n",
      " 1   Region            440 non-null    int64\n",
      " 2   Fresh             440 non-null    int64\n",
      " 3   Milk              440 non-null    int64\n",
      " 4   Grocery           440 non-null    int64\n",
      " 5   Frozen            440 non-null    int64\n",
      " 6   Detergents_Paper  440 non-null    int64\n",
      " 7   Delicassen        440 non-null    int64\n",
      "dtypes: int64(8)\n",
      "memory usage: 27.6 KB\n",
      "None\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Distribution of dependent variable: \n",
      "1    298\n",
      "0    142\n",
      "Name: Channel, dtype: int64\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Unique label in dependent variable: [0, 1]\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "How many data are missing: \n",
      "Channel             0\n",
      "Region              0\n",
      "Fresh               0\n",
      "Milk                0\n",
      "Grocery             0\n",
      "Frozen              0\n",
      "Detergents_Paper    0\n",
      "Delicassen          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data_statistics(df=data, response='Channel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Tuning XGBoost with Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify hyperparameters to try\n",
    "max_depth = [int(x) for x in np.linspace(3,7, num=3)]\n",
    "learning_rate = [0.1, 0.01, 0.001]\n",
    "subsample = [float(x) for x in np.linspace(0.5,1.0, num=3)]\n",
    "n_estimator = [int(x) for x in np.linspace(100,300, num=3)]\n",
    "alpha = [int(x) for x in np.linspace(3,9, num=3)]\n",
    "objective = ['binary:logistic']\n",
    "\n",
    "param = {\n",
    "    'max_depth': max_depth,\n",
    "    'learning_rate': learning_rate,\n",
    "    'subsample': subsample,\n",
    "    'n_estimator': n_estimator,\n",
    "    'alpha': alpha,\n",
    "    'objective': objective\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate xgboost classifier\n",
    "xgb_model = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n"
     ]
    }
   ],
   "source": [
    "start_gridsearch = time.time()\n",
    "#Instantiate GridSearch\n",
    "grid_search = GridSearchCV(estimator = xgb_model, \n",
    "                           param_grid = param, \n",
    "                           scoring=\"f1\", \n",
    "                           cv=5, \n",
    "                           n_jobs=-1, \n",
    "                           verbose=1)\n",
    "\n",
    "#fit training data to the xgboost gridsearch algorithm\n",
    "grid_search.fit(X_train, y_train)\n",
    "end_gridsearch = time.time()\n",
    "grid_time = (time.time() - start_gridsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.9716981132075472 \n",
      "Recall = 0.9716981132075472 \n",
      "f1 = 0.9716981132075472\n"
     ]
    }
   ],
   "source": [
    "_, precision_gridtrain, recall_gridtrain, f1_gridtrain = evaluate_train(grid_search)\n",
    "print(\"Precision = {} \\nRecall = {} \\nf1 = {}\".format(precision_gridtrain, recall_gridtrain, f1_gridtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.8829787234042553 \n",
      "Recall = 0.9651162790697675 \n",
      "f1 = 0.9222222222222223\n"
     ]
    }
   ],
   "source": [
    "_, precision_gridtest, recall_gridtest, f1_gridtest = evaluate_test(grid_search)\n",
    "print(\"Precision = {} \\nRecall = {} \\nf1 = {}\".format(precision_gridtest, recall_gridtest, f1_gridtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Tuning XGBoost with Randomized Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    }
   ],
   "source": [
    "start_randomsearch = time.time()\n",
    "#Instantiate RandomizedSearch\n",
    "random_search = RandomizedSearchCV(estimator = xgb_model, \n",
    "                           param_distributions = param, \n",
    "                           scoring=\"f1\", \n",
    "                           cv=5,\n",
    "                           random_state=35,\n",
    "                           n_jobs=-1, \n",
    "                           verbose=1)\n",
    "\n",
    "#fit training data to the xgboost gridsearch algorithm\n",
    "random_search.fit(X_train, y_train)\n",
    "end_randomsearch = time.time()\n",
    "random_time = (time.time() - start_randomsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.9619047619047619 \n",
      "Recall = 0.9528301886792453 \n",
      "f1 = 0.957345971563981\n"
     ]
    }
   ],
   "source": [
    "_, precision_randtrain, recall_randtrain, f1_randtrain = evaluate_train(random_search)\n",
    "print(\"Precision = {} \\nRecall = {} \\nf1 = {}\".format(precision_randtrain, recall_randtrain, f1_randtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.8804347826086957 \n",
      "Recall = 0.9418604651162791 \n",
      "f1 = 0.9101123595505618\n"
     ]
    }
   ],
   "source": [
    "_, precision_randtest, recall_randtest, f1_randtest = evaluate_test(random_search)\n",
    "print(\"Precision = {} \\nRecall = {} \\nf1 = {}\".format(precision_randtest, recall_randtest, f1_randtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Tuning XGBoost with Optuna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-27 16:36:41,440] A new study created in memory with name: no-name-35ffda45-ede3-47e5-9ddf-b43186a5f673\n"
     ]
    }
   ],
   "source": [
    "# define objective function \n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    The function returns the mean F1 score \n",
    "\n",
    "       Parameters: \n",
    "           trial: a process of evaluating an objective function\n",
    "       \n",
    "       Returns: f1-score\n",
    "\n",
    "    \"\"\" \n",
    "    n_estimators = trial.suggest_int('n_estimators', low=100, high=300, step=50)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 0.001, 0.1)\n",
    "    #learning_rate = trial.suggest_float('learning_rate', low=0.001, high=0.1,step=3)\n",
    "    subsample = trial.suggest_float('subsample', low=0.5, high=1.0, step=3)\n",
    "    max_depth = trial.suggest_int('max_depth', low=3, high=7, step=2)\n",
    "    alpha = trial.suggest_int('alpha', low=3, high=9, step=3)\n",
    "    objective = trial.suggest_categorical('objective', ['binary:logistic'])\n",
    "\n",
    "    xgb_c = XGBClassifier(n_estimator = n_estimators, \n",
    "                              objective = objective,\n",
    "                              learning_rate = learning_rate,\n",
    "                              subsample = subsample, \n",
    "                              max_depth = max_depth,\n",
    "                              alpha = alpha)\n",
    "    \n",
    "    score = cross_val_score(estimator=xgb_c,\n",
    "                            X=X_train,   \n",
    "                            y=y_train,\n",
    "                            scoring = 'f1',\n",
    "                            cv=5,\n",
    "                            n_jobs=-1).mean()\n",
    "    \n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(sampler=TPESampler(), direction='maximize')\n",
    "\n",
    "time_start = time.time()\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study.optimize(objective, n_trials=100)\n",
    "optuna_time = time.time() - time_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pprint(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best hyperparameters\n",
    "xgb_optuna = XGBClassifier(n_estimator = 250, \n",
    "                              objective = 'binary:logistic',\n",
    "                              learning_rate = 0.08262982765412646,\n",
    "                              subsample = 0.5, \n",
    "                              max_depth = 5,\n",
    "                              alpha = 3).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.9624413145539906 \n",
      "Recall = 0.9669811320754716 \n",
      "f1 = 0.9647058823529412\n"
     ]
    }
   ],
   "source": [
    "_, precision_optunatrain, recall_optunatrain, f1_optunatrain = evaluate_train(xgb_optuna)\n",
    "print(\"Precision = {} \\nRecall = {} \\nf1 = {}\".format(precision_optunatrain, recall_optunatrain, f1_optunatrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.8736842105263158 \n",
      "Recall = 0.9651162790697675 \n",
      "f1 = 0.9171270718232045\n"
     ]
    }
   ],
   "source": [
    "_, precision_optunatest, recall_optunatest, f1_optunatest = evaluate_test(xgb_optuna)\n",
    "print(\"Precision = {} \\nRecall = {} \\nf1 = {}\".format(precision_optunatest, recall_optunatest, f1_optunatest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7. Conclusion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_values = ['xgboost_grid', grid_time, precision_gridtrain, precision_gridtest, recall_gridtrain, recall_gridtest, f1_gridtrain, f1_gridtest]\n",
    "columns = ['models', 'Time Elapsed (s)', 'model_precision_train', 'model_precision_test', 'model_recall_train', 'model_recall_test', 'model_f1_train', 'model_f1_test']\n",
    "grid_results = pd.DataFrame([model_values], columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_values = ['xgboost_random', random_time, precision_randtrain, precision_randtest, recall_randtrain, recall_randtest, f1_randtrain, f1_randtest]\n",
    "columns = ['models', 'Time Elapsed (s)', 'model_precision_train', 'model_precision_test', 'model_recall_train', 'model_recall_test', 'model_f1_train', 'model_f1_test']\n",
    "random_results = pd.DataFrame([model_values], columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_values = ['xgboost_optuna', optuna_time, precision_optunatrain, precision_optunatest, recall_optunatrain, recall_optunatest, f1_optunatrain, f1_optunatest]\n",
    "columns = ['models', 'Time Elapsed (s)', 'model_precision_train', 'model_precision_test', 'model_recall_train', 'model_recall_test', 'model_f1_train', 'model_f1_test']\n",
    "optuna_results = pd.DataFrame([model_values], columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>models</th>\n",
       "      <th>Time Elapsed (s)</th>\n",
       "      <th>model_precision_train</th>\n",
       "      <th>model_precision_test</th>\n",
       "      <th>model_recall_train</th>\n",
       "      <th>model_recall_test</th>\n",
       "      <th>model_f1_train</th>\n",
       "      <th>model_f1_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Grid Search</th>\n",
       "      <td>xgboost_grid</td>\n",
       "      <td>14.147263</td>\n",
       "      <td>0.971698</td>\n",
       "      <td>0.882979</td>\n",
       "      <td>0.971698</td>\n",
       "      <td>0.965116</td>\n",
       "      <td>0.971698</td>\n",
       "      <td>0.922222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Optuna</th>\n",
       "      <td>xgboost_optuna</td>\n",
       "      <td>12.232657</td>\n",
       "      <td>0.962441</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.966981</td>\n",
       "      <td>0.965116</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>0.917127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Randomized Search</th>\n",
       "      <td>xgboost_random</td>\n",
       "      <td>0.582441</td>\n",
       "      <td>0.961905</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.952830</td>\n",
       "      <td>0.941860</td>\n",
       "      <td>0.957346</td>\n",
       "      <td>0.910112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           models  Time Elapsed (s)  model_precision_train  \\\n",
       "Grid Search          xgboost_grid         14.147263               0.971698   \n",
       "Optuna             xgboost_optuna         12.232657               0.962441   \n",
       "Randomized Search  xgboost_random          0.582441               0.961905   \n",
       "\n",
       "                   model_precision_test  model_recall_train  \\\n",
       "Grid Search                    0.882979            0.971698   \n",
       "Optuna                         0.873684            0.966981   \n",
       "Randomized Search              0.880435            0.952830   \n",
       "\n",
       "                   model_recall_test  model_f1_train  model_f1_test  \n",
       "Grid Search                 0.965116        0.971698       0.922222  \n",
       "Optuna                      0.965116        0.964706       0.917127  \n",
       "Randomized Search           0.941860        0.957346       0.910112  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stack table \n",
    "final_results = grid_results.append(random_results).append(optuna_results)\n",
    "final_results.index = ['Grid Search', 'Randomized Search', 'Optuna']\n",
    "final_results.sort_values('model_f1_test', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of performance, when evaluating based on the F1-score, it became evident that XGBoost, with both grid search and Optuna, demonstrated the most outstanding performance. However, when we take into account not only model performance but also runtime efficiency, Optuna emerged as the top-performing method.\n",
    "\n",
    "Nevertheless, if there's a willingness to make a slight compromise on model performance and prioritize runtime efficiency, XGBoost with randomized search outperformed both grid search and Optuna.\n",
    "\n",
    "To sum it up, the choice of hyperparameter tuning method is heavily contingent on the specific context. For instance, in a production environment where time and resources are at a premium, grid search may prove too resource-intensive due to its exhaustive exploration of hyperparameter combinations. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
